{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector quantisation of one grey scale image\n",
    "This tutorial will show an example of vector quantisation applied to a grey scale image (*cameraman.tif*) where pairs of pixels are vector quantised with 8 bits for each pair, roughly amounting to 4 bits per single pixel. The coding efficiency of such quantisation scheme will be compared with a uniform scalar quantisation scheme which quantises each pixel with the same amount of bits (i.e. 4). Given that both quantisers (i.e. scalar and vector) operate at the same bits per pixel, we'll measure the distortion in terms of Peak-Signal-to-Noise-Ratio (PSNR) and comment the objective and subjetive visual quality. The main goal of this training is to provide the reader with a practical example of vector quantisation, most notably how the generalised Lloyd-Max algorithm could be implemented. For a thorough treatment of the fundamentals of vector quantisation, the interested reader is referred to the following textbooks:\n",
    " * Allen Gersho and Robert M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Press, 732 pages, 1992.\n",
    " * David S. Taubman and Micheal W. Marcellin, \"JPEG 2000: Image compression fundamentals, standards and practice\", Kluwer Academic Press, 773 pages, 2002.\n",
    "\n",
    "For vector quantisation, our pairs are constituted by the pixels belonging to two consecutive rows. This is shown in the following figure.\n",
    "\n",
    "<img src=\"vectors.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The overall processing can be summarised with the following three main steps:\n",
    " * **Step 1**: Select a subset of vectors which will constitute the so-called Training Set (*TS*) and use it to design the reproduction levels for all vectors to be quantised (the so-called codebook).\n",
    " * **Step 2**: Derive the reproduction levels $l_i$ using the generalised Lloyd-Max algorithm over the TS found earlier.\n",
    " * **Step 3**: Perform the actual vector quantisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Selection of the training set\n",
    "This step receives as input a gray scale image and returns the aforementioned Training Set (*TS*), constituted by a subset of the pairs of pixels associated with the whole image (i.e. our vectors). More precisely, we'll subsample all pairs of adjacent image pixels by a factor of 4 and insert such pairs in *TS*. Note that the subsampling factor is arbitrary but its value leads to a trade-off between coding efficiency and complexity. In fact, large subsampling factors will speed up the design of the reproduction levels (i.e. **Step 2**) but will result in lower coding efficiency as the levels have been designed on a set of pixel pairs which may not be representative of the image statistics. Conversely, a smaller subsampling factor, will increase coding efficiency given that now more pixels are included in the design of the codebook. The price to pay for this is an increase in the encoder's complexity. The following image depicts the selection of vectors to be included in the training set, while the code cell below implements such selection.\n",
    "\n",
    "<img src=\"training-set.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vector quantiser bits\n",
    "vq_bits = 8\n",
    "vq_levels = 2**vq_bits\n",
    "\n",
    "image = cv2.imread('../../input-data/cameraman.tif', cv2.IMREAD_UNCHANGED)\n",
    "rows, cols = image.shape[0:2]\n",
    "\n",
    "sampling_ratio, vector_height, vector_width = (4, 2, 1)\n",
    "total_training_samples = (rows * cols) // (vector_height * sampling_ratio * vector_width * sampling_ratio)\n",
    "training_set = np.zeros((vector_height * vector_width, total_training_samples), np.int32)\n",
    "\n",
    "k = 0\n",
    "for r in range(0, rows, vector_height * sampling_ratio):\n",
    "    for c in range(0, cols, vector_width*sampling_ratio):\n",
    "        training_vector = image[r:r + vector_height, c:c + vector_width]\n",
    "        training_set[:, k] = training_vector.flatten()\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Derivation of the reproduction levels\n",
    "Once the training set is available, the reproduction levels can be derived by applying the generalised Lloyd-Max algorithm (see the references listed above for more details). Accordingly, the initial reproduction levels will be set equal to some vectors belonging to the training set. It is important to note at this point that the choice of the initial value for the reproduction level will primarily impact on the convergence speed of the Lloyd-Max algorithm. So if we had more information about the image statistics (e.g. we know that the image has a bimodal histogram) we could reduce the number of iterations by properly selecting the values associated with the two peaks in the histogram. For this example we'll select the initial value by sampling the training set calculated by a factor $r = \\frac{|TS|}{2^{qb}}$, where $qb$ in our example is equal to 8 bits per vector and $|\\cdot|$ denotes the number of vectors included in the training set. Let's denote the set of initial reproduction levels as $L_{init}$, the Llyod-Max algorithm will take $L_{init}$ as input parameter along with the training set vectors. The output of the algorithm will be the set of reproduction levels $L_{final}$ containing all reproduction levels which minimise the overall Mean Square Error (MSE) between the vectors in *TS* and their vector quantised counterparts.\n",
    "\n",
    "We can summarise the generalised Lloyd-Max algorithm with the following sequence of ordered steps:\n",
    " 1. Set $L_{final} = L_{init}$.\n",
    " 1. For each vector $v_i$ in the training set, find the reproduction level $l_i \\in L_{final}$ which minimises the square error $e^2 = (v_i - l_i)^2$.\n",
    " 1. Add the value of $e^2$ to variable $SE$ which stores the overall square error for the current iteration.\n",
    " 1. Update $L_{final}$ as $L_{final} = L_{final} / H$, where $H$ denotes a 1D array having each $i$-th element containing the number of times $l_i$ has been selected as the closest reproduction level for a given $v_i$ in the training set. If a given $l_i$ has never been selected, then substitute it by randomly choosing another vector from the training set.\n",
    " 1. If $SE$ hasn't decreased by a factor $\\epsilon$ stop, else go to Step 2.\n",
    " \n",
    " The following Python code cell implements such iterative procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ts_sampling_ratio = total_training_samples // vq_levels\n",
    "reproduction_levels = training_set[:, ts_sampling_ratio-1::ts_sampling_ratio].astype(np.float64)\n",
    "\n",
    "last_iteration_mse = 1e6\n",
    "epsilon = 1e-3\n",
    "iteration = 0\n",
    "delta_mse = 1.0\n",
    "\n",
    "print(\"Step\\tMSE\\tvariation\")\n",
    "while delta_mse > epsilon:\n",
    "    levels_accumulator = np.zeros((vector_height * vector_width, vq_levels), np.float64)\n",
    "    levels_hit_cnt = np.zeros(vq_levels, np.int32)\n",
    "    MSE = 0.0\n",
    "\n",
    "    # Step 2: For each vector vi in the training set, find the reproduction level li which minimises \n",
    "    # the square error\n",
    "    for i in range(total_training_samples):\n",
    "        V = training_set[:, i]\n",
    "        dV = np.dot(V.T, V) + np.sum(np.square(reproduction_levels), axis=0) - 2*np.dot(V.T, reproduction_levels)\n",
    "        square_error = np.min(dV)\n",
    "        l_start_idx = np.argmin(dV)\n",
    "        levels_accumulator[:, l_start_idx] += V\n",
    "        levels_hit_cnt[l_start_idx] += 1\n",
    "        MSE += square_error\n",
    "\n",
    "    MSE /= total_training_samples * vector_height * vector_width\n",
    "\n",
    "    # Step 3: Update Lfinal as Lfinal = Lfinal / H\n",
    "    for i in range(vq_levels):\n",
    "        if levels_hit_cnt[i]:\n",
    "            reproduction_levels[:, i] = levels_accumulator[:, i] / levels_hit_cnt[i]\n",
    "        else:\n",
    "            random_idx = max(1, int(rnd.random()*total_training_samples))\n",
    "            reproduction_levels[:, i] = training_set[:, random_idx]\n",
    "\n",
    "    delta_mse = (last_iteration_mse - MSE) / MSE\n",
    "    print(f\"{iteration}\\t{MSE}\\t{delta_mse}\")\n",
    "    iteration += 1\n",
    "    last_iteration_mse = MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Worth noting from the code cell above how the selection of $l_i$ from Step 2 of the generalised Lloyd-Max algorithm is implemented. In principle, finding $l_i$ which minimises the square error with the current vector $v_i$ can be done by looping through all reproduction levels, compute such square error and pick the one which minimises it. However, we can compact the code by noting that what we're doing is indeed the following:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "e^2 = \\lvert\\lvert L_{final} - v_i\\rvert\\rvert^2 = L_{final}\\cdot L_{final}^t - 2*L_{final}^t\\cdot v_i + v_i\\cdot v_i^t,\n",
    "$$\n",
    "\n",
    "where superscript $^t$ denotes the transpose operator and $\\lvert\\lvert \\cdot\\rvert\\rvert^2$ is the $L^2$ norm. Given that our data are stored in **numpy** arrays, dot products and element wise operations such as sum and subtraction are easily implemented and built in as either overloaded operators or interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Actual vector quantisation over input image\n",
    "Now that the optimal reproduction levels have been found by the generalised Lloyd-Max algorithm, it is time to perform actual vector quantisation. The processing is similar to what we did above when deriving the optimal reproduction levels. In fact, this time we'll loop through all vectors associated with the *cameraman.tif* image and, for each one, $v_i$, the reproduction level $l_i$ from $L_{final}$ which minimises the square error $e^2 = (v_i - l_i)^2$, will be selected. Vector $l_i$ will be then placed at the same spatial location of $v_i$ and the process can move to the next $v_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "reproduction_levels = np.round(reproduction_levels).astype(np.int32)\n",
    "square_sum_level = np.sum(np.square(reproduction_levels), axis=0)\n",
    "\n",
    "image_vq = np.zeros((rows, cols), np.uint8)\n",
    "for r in range(0, rows, vector_height):\n",
    "    for c in range(0, cols, vector_width):\n",
    "        V = image[r:r + vector_height, c:c + vector_width].flatten()\n",
    "        dV = np.dot(V.T, V) + square_sum_level - 2*np.dot(V.T, reproduction_levels)\n",
    "        l_start_idx = np.argmin(dV)\n",
    "        image_vq[r:r + vector_height, c:c + vector_width] =\\\n",
    "        np.reshape(reproduction_levels[:, l_start_idx], (vector_height, vector_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison with scalar quantisation\n",
    "The last operation we need to perform is to quantise *cameraman.tif* with a quantiser having $qb = 4$, that is 4 bits per pixel. Over the images obtained with scalar and vector quantisation, we'll then compute the Peak-Signal-to-Noise-Ratio (PSNR) and express it in decibel [dB] according to the following formula:\n",
    "\n",
    "$$\n",
    "PSNR(I,\\hat{I}) = 10\\cdot\\log_{10}\\left(\\frac{M^2}{E\\left[\\lvert\\lvert I - \\hat{I}\\rvert\\rvert^2\\right]}\\right) [dB],\n",
    "$$\n",
    "\n",
    "where $\\hat{I}$ denotes the image quantised with either scalar or vector quantisation, $M$ is the maximum value allowed for image $I$, that is 255 with an 8 bit per pixel image and finally $E[\\cdot]$ denotes the expectation operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sq_bits = vq_bits / vector_height / vector_width\n",
    "Q = 256 // 2**sq_bits\n",
    "image_sq = np.round(image / Q).astype(np.int32) * Q\n",
    "\n",
    "mse_vq = np.mean(np.square(image - image_vq))\n",
    "mse_sq = np.mean(np.square(image - image_sq))\n",
    "psnr_vq = 10.0*np.log10(255.0**2 / mse_vq)\n",
    "psnr_sq = 10.0*np.log10(255.0**2 / mse_sq)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1, 3, 1), plt.imshow(image, cmap='gray'), plt.title('Original image')\n",
    "plt.subplot(1, 3, 2), plt.imshow(image_vq, cmap='gray'), plt.title(f\"Vector quantised image (PSNR = {psnr_vq:.2f} [dB])\")\n",
    "plt.subplot(1, 3, 3), plt.imshow(image_sq, cmap='gray'), plt.title(f\"Scalar quantised image (PSNR = {psnr_sq:.2f} [dB])\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As we may note, the image resulting from scalar quantisation shows noticeable banding artefacts. Quality is significantly better for the vector quantised image which not only improves the PSNR by almost 4 dBs but also shows less artefacts.\n",
    "\n",
    "This example shows a compelling case for the use of vector quantisation: in fact, the vectors considered (i.e. pair of adjacent image pixels) show a correlation which would make their distribution on a scatter plot aligned to the 45 degree straight line. Such a correlation is efficiently exploited by vector quantisation whereby the generalised Lloyd-Max algorithm places the reproduction levels along the joint probability mass function. Scalar quantisation doesn't consider this pair-based correlation, hence places all reproduction levels as to span all possible range of values (even those which would never appear in the image statistics).\n",
    "\n",
    "# Concluding remarks\n",
    "We have presented a simple implementation of the generalised Lloyd-Max algorithm with application to image coding via vector quantisation. We have verified that vector quantisation is indeed a better alternative to scalar quantisation when the input data show some degree of correlation (or redundancy). Accordingly, if the transmitter (i.e. the encoder) can bear some additional complexity, vector quantisation can constitute an attractive alternative. Worth noting that we didn't considered to apply entropy coding on top of the resulting quantisation cells indexes: this would still reduce the coding rate given there will be some inter symbol redundancy to exploit with a coding scheme such as run length encoding.\n",
    "\n",
    "It is also worth to mention that sometimes vector quantisation is referred as palette coding and a good example of design for the case of screen content and RGB images is the palette mode from the H.265/HEVC (V3) and H.266/VVC standards.\n",
    "\n",
    "We shall also provide the reader with some ideas on the extension of the vector quantisation scheme presented in this tutorial:\n",
    " * Consider colour images. Some design choices and aspects to address would be wether the input data are considered in the RGB or a YCbCr colour space. The former might save in complexity since no colour transform is required but would not allow for an effective perceptual quantisation. Another aspect is whether to treat each image plane separately or jointly. The latter might bring benefits in terms of coding efficiency.\n",
    " * Consider region based vector quantisation. Here the images is broken up into square regions and a different codebook is derived for each region. This will allow for parallel encoding and decoding, along with a more content adaptive coding scheme which, in this case, would get closer the palette mode of the H.265/HEVC and H.266/VVC standards.\n",
    "\n",
    "Finally, although we pointed out at the encoder's complexity as a limiting factor for vector quantisation, we should remind that in case of a region-based approach, GPU implementation of k-means algorithms (another way of optimising the codebook) will speed up compression. At the receiver side, the decoding process is a simple read from the bitstream and look up operation to write the pixels to the output buffer."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "baa79462166bc77c8f9369368ac63f97af70b25c4f86bb0e7e04757a425fec63"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
