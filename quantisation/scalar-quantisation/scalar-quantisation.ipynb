{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar uniform quantisation of random variables\n",
    "This tutorial considers scalar quantisation implemented using a uniform quantiser and applied over random variables with different Probability Mass Functions (PMFs). In particular we will consider uniform- and Gaussian-distributed random variables so to comment on the optimality of such a simple quantiser.\n",
    "\n",
    "## Preliminary remarks\n",
    "Quantisation is an irreversible operation which reduces the precision used to represent our data to be encoded. Such a precision reduction translates into less bits used to transmit the information. Accordingly, the whole dynamic range associated with the input data ($X$) is divided into intervals denoted as *quantisation bins*, each having a given width. Each quantisation bin $b_i$ is also associated with its reproduction level $l_i$ which corresponds to the value used to represent all original data values belonging to $b_i$. From this description, it is easy to realise why quantisation is an irreversible process: it is indeed a *many-to-one* mapping, hence after a value $x$ is quantised it cannot be recovered. Usually a quantiser is associated with its number of bits $qb$ which determines the number of reproduction levels, given as $2^{qb}$. If scalar (1D) quantities are presented to the quantiser as input, then we talk about *scalar quantisation* (i.e. the subject of this tutorial) if group of samples are considered together as input to the quantiser, we talk about *vector quantisation*.\n",
    "\n",
    "During encoding the quantiser will output the index $i$ of each quantisation bin $b_i$ where each input sample belongs to. The decoder will receive these indexes and write to the output the corresponding reproduction level $l_i$. The mapping $\\{b_i \\leftrightarrow l_i\\}$ must be known at the decoder side. Working out the optimal partitioning of the input data range (i.e. the width of each $b_i$) and the associated set of $\\{l_i\\}$ can be a computational intensive process, although it can provide significant gains in the overall rate distortion performance of our coding system.\n",
    "\n",
    "A widely and well-known used quantiser is the so-called *uniform quantiser*, characterised by having each $b_i$ with the same width and the reproduction level $l_i$ placed at the mid-value of the quantisation bin, that is:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "l_i = \\frac{b_i + b_{i+1}}{2}.\n",
    "$$\n",
    "\n",
    "The width of each quantisation bin is usually denoted as the quantisation step $\\Delta$, given as:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\Delta = \\frac{\\max(X) - \\min(X)}{2^{qb}}.\n",
    "$$\n",
    "\n",
    "Using the Mean Square Error (MSE) as distortion measure for the quantisation error ($e$) and considering the input data ($X$) to have a uniform PMF, the variance of $e$, $\\sigma^2_e$ is given by:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\sigma^2_e = \\frac{\\Delta^2}{12}.\n",
    "$$\n",
    "\n",
    "If $X$ ranges in $\\left[-\\frac{M\\Delta}{2},\\frac{M\\Delta}{2}\\right]$ and if we consider the Signal-to-Noise-Ratio (SNR) as alternative measure to express the reproduction quality, then we have the so-called ***six dB rule***:\n",
    "\n",
    "$$\n",
    "SNR = 6 \\cdot qb\\quad[dB],\n",
    "$$\n",
    "\n",
    "That is, each bit added to increase the number of reproduction levels will provide a 6 dB improvement to our reconstructed quality. More details about rate distortion theory and quantisation are provided in these two good references:\n",
    " * Allen Gersho and Robert M. Gray. Vector Quantization and Signal Compression. Kluwer Academic Press, 732 pages, 1992.\n",
    " * David S. Taubman and Micheal W. Marcellin, \"JPEG2000: Image compression fundamentals, standards and practice\", Kluwer Academic Press, 773 pages, 2002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate distortion performance of a uniform quantiser\n",
    "We demonstrate now the rate-distortion performance of a uniform quantiser and see how this resembles the six dB rule. We also note from the remarks above that a uniform quantiser is a sort of low complexity solution to quantisation. In fact, encoding results in a simple integer precision division by $\\Delta$ rather than a comparison of each input data with the different quantisation bins' extrema. Moreover, at the decoder side, the only additional information one would require is only $\\Delta$. Accordingly, it is interesting to verify whether a uniform quantiser is able to attain the six dB rule also for other PMFs, most notably knowing that when transformation is used in our coding scheme, the distribution of coefficients tends to be more Laplacian or Gaussian.\n",
    "\n",
    "The following Python code cell will generate two input data: one with uniform and another with Gaussian PMF (zero mean and variance $\\sigma^2$ equal to four). Uniform quantisation is applied to both inputs and the SNR is computed on the reconstructed values. A plot of the rate distortion perform is shown along the straight line associated with the 6 dB rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Total samples\n",
    "N = 1000\n",
    "\n",
    "# Quantiser's bits\n",
    "qb = np.arange(0, 8, 1)\n",
    "\n",
    "# Generate a random variable uniformly distributed in [0, 255]\n",
    "X = np.round(255*rnd.rand(N, 1)).astype(np.int32)\n",
    "var_X = np.var(X)\n",
    "\n",
    "# Generate a random Gaussian variable with mean 128 and variance 4\n",
    "Xg = 2.0*rnd.randn(N, 1) + 128\n",
    "var_Xg = np.var(Xg)\n",
    "B = np.max(Xg) - np.min(Xg)\n",
    "\n",
    "snr_data = np.zeros(len(qb))\n",
    "snr_data_g = np.zeros(len(qb))\n",
    "\n",
    "for i, b in enumerate(qb):\n",
    "    levels = 2**b\n",
    "    Q = 256.0 / float(levels)\n",
    "    Qg = B / float( levels)\n",
    "    Y = Q * np.round(X / Q)\n",
    "    Yg = Qg*np.round(Xg / Qg)\n",
    "    mse = np.mean(np.square(X - Y))\n",
    "    mse_g = np.mean(np.square(Xg - Yg))\n",
    "    snr_data[i] = 10*np.log10(var_X / mse)\n",
    "    snr_data_g[i] = 10*np.log10(var_Xg / mse_g)\n",
    "\n",
    "six_dB_rule = 6.0 * qb\n",
    "\n",
    "# Plot the results and verify the 6dB rule\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(qb, snr_data, 'b-o', label='Uniform quantiser with uniform variable')\n",
    "plt.xlabel('Quantiser bits', fontsize=16)\n",
    "plt.ylabel('Signal-to-Noise-Ratio SNR [dB]', fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(qb, snr_data_g, 'k-+', label='Uniform quantiser with Gaussian variable')\n",
    "plt.plot(qb, six_dB_rule, 'r-*', label='Six dB rule')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the uniform quantiser applied over a uniformly distributed input provides a rate-distortion performance which follows the six dB rule. Conversely, when the input is Gaussian, then the performance is offset by approximately 4 dB. Such a suboptimal performance is due to the fact that the reproduction levels are placed at the mid-point of each interval, which for a uniform PMF is absolutely fine since each value in a given bin $b_i$ has equal chance to appear. This is not the case for a Gaussian PMF where in each bin some values have higher chance to appear than others. Accordingly, it would make sense to place the reproduction levels around those values which are more likely to appear. The procedure which does this automatically is the subject of the next section of our tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards an optimal quantiser: The Lloyd-Max algorithm\n",
    "As mentioned above, we want to find a procedure which adjusts the reproduction levels to fit the underlying PMF of the data. In particular, by using again the MSE as distortion measure, one can show that the reproduction levels which minimise the MSE in each quantisation bin is given by:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "l_i = E[X|X\\in b_i]= \\frac{\\sum_{x_i\\in b_i}x_i\\cdot P_X(x_i)}{\\sum_{x_i\\in b_i}P_X(x_i)},\n",
    "$$\n",
    "\n",
    "where $P_X$ denotes the PMF of the input $X$. The condition above is usually denoted as *centroid condition* and, for a continuos variable, becomes:\n",
    "$$\n",
    "\\large\n",
    "l_i = E[X|X\\in b_i]= \\frac{\\int_{x\\in b_i}x\\cdot f_X(x)}{\\int_{x\\in b_i}f_X(x)},\n",
    "$$\n",
    "\n",
    "where now $f_X$ denotes the Probability Density Function (PDF). We note that the centroid condition above requires to know the partitioning of the input data into quantisation bins $b_i$. Given that we do not know beforehand what such a partitioning would look like, we could assume an initial partitioning with equal width (as for a uniform quantiser) and then compute the reproduction levels according to the centroid condition above. Once all reproduction levels have been computed, we can derive a new set of quantisation bins whereby the extrema of each bin are given by the mid point of the reproduction levels computed previously. We then compute a new set of reproduction levels and continue to iterate until convergence is reached. More precisely, the following pseudo code represents the workflow we just described in plain text:\n",
    "\n",
    " * k = 0\n",
    " * set $b_i^k$ equal to the bins associated with a uniform quantiser with $qb$ bits\n",
    " * apply uniform quantisation over the input data, compute the associated MSE and set it to $MSE_{old}$\n",
    " * set $\\gamma$ = $\\infty$\n",
    " * while $\\gamma > \\epsilon$:\n",
    "   * compute $l_i^k$ using the centroid condition for each bin $b_i^k$\n",
    "   * derive the new quantisation bins as $b_i^{k+1} = \\frac{l_{i}^k + l_{i+1}^k}{2}$\n",
    "   * apply the quantiser derived by these new bins and reproduction levels and compute the MSE\n",
    "   * compute $\\gamma = \\frac{MSE_{old} - MSE}{MSE_{old}}$\n",
    "   * set $k = k + 1$\n",
    "\n",
    "Where $\\epsilon$ denotes a given tolerance threshold. The iterative produce described above is also known as the [Lloyd-Max algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm). The next code cell will provide you with an implementation of the Lloyd-Max algorithm, which is conveniently wrapped up as a function so we can then use it to compare its rate-distortion performance with the uniform quantiser analysed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple\n",
    "\n",
    "from nptyping import NDArray\n",
    "\n",
    "def lloydmax(X: NDArray[(Any), np.float64], qb: int) -> Tuple[List[float], List[float], NDArray[(Any), np.float64]]:\n",
    "    levels = 1 << qb\n",
    "    delta = (np.max(X) + 1 - np.min(X)) / levels\n",
    "\n",
    "    # Quantisation bins\n",
    "    bins = np.array([np.min(X) + float(i * delta) for i in range(levels + 1)], np.float64)\n",
    "\n",
    "    # Reproduction levels\n",
    "    rl = (bins[:levels] + bins[1:levels + 1]) / 2\n",
    "    \n",
    "    # Add a small increment to include the max value of X\n",
    "    bins[-1] += 0.1\n",
    "\n",
    "    # Codebook initialization with a uniform scalar quantiser\n",
    "    XQ = np.zeros(X.shape)\n",
    "    for i in range(rl.size):\n",
    "        index = (bins[i] <= X) & (X < bins[i + 1])\n",
    "        XQ[index] = rl[i]\n",
    "\n",
    "    error = np.square(X - XQ)\n",
    "    MSE_old = np.average(error)\n",
    "\n",
    "    epsilon, variation, step = 1e-5, 1, 1\n",
    "\n",
    "    # Lloyd-Max Iteration over all decision thresholds and reproduction levels\n",
    "    bins_next, rl_next = np.zeros(bins.shape), np.zeros(rl.shape)\n",
    "    while variation > epsilon:\n",
    "        # Loop over all reproduction levels in order to adjust them wrt\n",
    "        # centroid condition\n",
    "        for i in range(levels):\n",
    "            index = (bins[i] <= X) & (X < bins[i + 1])\n",
    "            if np.all(~index):  # empty decision threshold, relative reprodution level will be the same for the next step\n",
    "                rl_next[i] = rl[i]\n",
    "            else:  # centroid condition\n",
    "                rl_next[i] = np.sum(X[index]) / np.sum(index)\n",
    "\n",
    "        # New decision threshold: they are at the mid point of two\n",
    "        # reproduction levels\n",
    "        bins_next[1:levels] = (rl_next[:levels - 1] + rl_next[1:levels]) / 2\n",
    "        bins_next[0], bins_next[-1] = bins[0], bins[-1]\n",
    "\n",
    "        # New MSE calculation\n",
    "        XQ[:] = 0\n",
    "        for i in range(rl_next.size):\n",
    "            index = (bins_next[i] <= X) & (X < bins_next[i + 1])\n",
    "            XQ[index] = rl_next[i]\n",
    "\n",
    "        MSE = np.average(np.square(X - XQ))\n",
    "        variation = (MSE_old - MSE) / (MSE_old)\n",
    "\n",
    "        # Swap the old variables with the new ones\n",
    "        bins, rl, MSE_old = bins_next, rl_next, MSE\n",
    "        step += 1\n",
    "\n",
    "    return bins, rl, XQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above contains some comments to help the reader understand the flow. We are now ready to try this non uniform quantiser and measure its performance. The following code cell in Python will run the Lloyd-Max quantiser for each of the tested quantiser bit values and compute its associated SNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_data_lm = np.zeros((len(qb)))\n",
    "\n",
    "for i, b in enumerate(qb):\n",
    "    _, _, xq_lm = lloydmax(Xg, b)\n",
    "    mse = np.average(np.square(Xg - xq_lm))\n",
    "    snr_data_lm[i] = 10 * np.log10(var_Xg / mse)\n",
    "\n",
    "# Plot the results, including the 6dB rule\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(qb, snr_data, 'b-o', label='Uniform quantiser applied to uniform PMF')\n",
    "plt.xlabel('Quantiser bits', fontsize=16)\n",
    "plt.ylabel('Signal-to-Noise-Ratio SNR [dB]', fontsize=16)\n",
    "plt.grid()\n",
    "plt.plot(qb, snr_data_g, 'k-+', label='Uniform quantiser applied to Gaussian PMF')\n",
    "plt.plot(qb, snr_data_lm, 'g-x', label='Lloyd-Max quantiser applied to Gaussian PMF')\n",
    "plt.plot(qb, six_dB_rule, 'r-*', label='Six dB rule')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from the graph above that the Lloyd-Max algorithm starts by providing a better SNR performance at low bitrates and then tends to sit on the same performance of the uniform quantiser when applied to a Gaussian variable. This result might be surprising at first sight but it is actually not. In fact, as the number of quantiser bits gets higher, the quantisation step of the Lloyd-Max quantiser gets smaller and the PMF enclosed in each quantisation bin resembles a uniform one. In that case, the best the Lloyd-Max algorithm can do is to place all reproduction levels at the mid-point of the quantisation bin, which is exactly what a uniform quantiser would do. Finally, we also note that the Lloyd-Max algorithm would require to send the reproduction levels and quantisation bins, thus some additional rate needs to be added to the bits used by the quantiser.\n",
    "\n",
    "## Concluding remarks\n",
    "In this short tutorial we have investigated the rate-distortion performance of two types of scalar quantiser when applied to random variables with a given probability mass function. We showed how a uniform quantiser follows the six dB rule when it is applied to a random variable uniformly distributed but it is sub-optimal in the case of a Gaussian PMF. We then introduced the Lloyd-Max algorithm which provides a better rate-distortion performance, most notably when the number of bits allocated to the quantiser is small. The price to pay for this improved rate-distortion tradeoff is the additional complexity associated with the iterative Lloyd-Max procedure."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "baa79462166bc77c8f9369368ac63f97af70b25c4f86bb0e7e04757a425fec63"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
